{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marsgav/TowardTacklingEuphemisms/blob/main/MGEuphemisms_AutoPhrase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Toward Processing Euphemisms in Natural Language Processing\n",
        "Martha Gavidia\n",
        "APLN 606"
      ],
      "metadata": {
        "id": "ryt4-twp9rvH"
      },
      "id": "ryt4-twp9rvH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains the code used to run the experiment in \"Toward Processing Euphemisms in Natural Language Processing\" for APLN 606.\n",
        "\n",
        "The code base comes from Searching for PETs: Using Distributional and Sentiment-based Methods for Finding Potentially Euphemistic PETs \n",
        "\n",
        "Lee P., Gavidia M., Feldman A. and J. Peng. “Searching for PETs: Using Distributional and Sentiment-Based Methods to Find Potentially Euphemistic Terms”. In Proceedings of the 2nd Workshop on Understanding Implicit and Underspecified Language, NAACL 2022, Seattle\n",
        "\n",
        "The changes in this notebook come at the phrase extraction stage, where Lee et. all use Phrases for finding phrases within text, this notebook experiments with Autophrase.  Used in conjunction with spacy's PhraseMatcher, the algorithm is then run with this new phrase extraction method."
      ],
      "metadata": {
        "id": "ZAnJT87D9vF6"
      },
      "id": "ZAnJT87D9vF6"
    },
    {
      "cell_type": "markdown",
      "id": "78f29660-07f3-48b4-a723-8178fef72b54",
      "metadata": {
        "id": "78f29660-07f3-48b4-a723-8178fef72b54"
      },
      "source": [
        "# Load Euphemism Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0137157-4040-433b-8b43-52c1a946cca5",
      "metadata": {
        "id": "c0137157-4040-433b-8b43-52c1a946cca5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "euph_corpus = pd.read_csv('Euphemism_Corpus_2-24.csv', index_col=0, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "717114fa-2511-4cbb-ba21-587e2e2f0f40",
      "metadata": {
        "id": "717114fa-2511-4cbb-ba21-587e2e2f0f40",
        "outputId": "fd38cd3b-42ba-4e1b-9538-e45e4631b2bf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keyword</th>\n",
              "      <th>edited_text</th>\n",
              "      <th>is_euph</th>\n",
              "      <th>category</th>\n",
              "      <th>type</th>\n",
              "      <th>euph_status</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tinkle</td>\n",
              "      <td>We're just getting back what was TAKEN from us...</td>\n",
              "      <td>1</td>\n",
              "      <td>bodily functions</td>\n",
              "      <td>tinkle</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>We're just getting back what was TAKEN from us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tinkle</td>\n",
              "      <td>I think AB390 will pass next year now that the...</td>\n",
              "      <td>1</td>\n",
              "      <td>bodily functions</td>\n",
              "      <td>tinkle</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>I think AB390 will pass next year now that the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>undocumented immigrants</td>\n",
              "      <td>Singled Out Think Like a Man, the new movie ba...</td>\n",
              "      <td>1</td>\n",
              "      <td>politics</td>\n",
              "      <td>undocumented immigrant</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>Anything but Secure A federal program designed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>undocumented immigrants</td>\n",
              "      <td>Not to be outdone, Sen. Rand Paul (R-Ky. ), so...</td>\n",
              "      <td>1</td>\n",
              "      <td>politics</td>\n",
              "      <td>undocumented immigrant</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>In a post-election interview with POLITICO Pau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>undocumented immigrants</td>\n",
              "      <td>The law has also galvanized the growing immigr...</td>\n",
              "      <td>1</td>\n",
              "      <td>politics</td>\n",
              "      <td>undocumented immigrant</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>Aside from undocumented immigrants the America...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   keyword                                        edited_text  \\\n",
              "0                   tinkle  We're just getting back what was TAKEN from us...   \n",
              "1                   tinkle  I think AB390 will pass next year now that the...   \n",
              "2  undocumented immigrants  Singled Out Think Like a Man, the new movie ba...   \n",
              "3  undocumented immigrants  Not to be outdone, Sen. Rand Paul (R-Ky. ), so...   \n",
              "4  undocumented immigrants  The law has also galvanized the growing immigr...   \n",
              "\n",
              "   is_euph          category                    type  euph_status  \\\n",
              "0        1  bodily functions                  tinkle  always_euph   \n",
              "1        1  bodily functions                  tinkle  always_euph   \n",
              "2        1          politics  undocumented immigrant  always_euph   \n",
              "3        1          politics  undocumented immigrant  always_euph   \n",
              "4        1          politics  undocumented immigrant  always_euph   \n",
              "\n",
              "                                            sentence  \n",
              "0  We're just getting back what was TAKEN from us...  \n",
              "1  I think AB390 will pass next year now that the...  \n",
              "2  Anything but Secure A federal program designed...  \n",
              "3  In a post-election interview with POLITICO Pau...  \n",
              "4  Aside from undocumented immigrants the America...  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "euph_corpus.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17a8998d-a922-4b46-9f6c-866dcb31b93f",
      "metadata": {
        "id": "17a8998d-a922-4b46-9f6c-866dcb31b93f"
      },
      "outputs": [],
      "source": [
        "def preprocess(s):\n",
        "    s = s.strip()\n",
        "    s = re.sub(r'(##\\d*\\W)|<\\w>|,|;|:|--|\\(|\\)|#|%|\\\\|\\/|\\.|\\*|\\+|@', '', s)\n",
        "    s = re.sub(r'\\s\\s+', ' ', s)\n",
        "    s = s.lower()\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e964241f-ee69-41d6-aec8-97de5dc1b00f",
      "metadata": {
        "id": "e964241f-ee69-41d6-aec8-97de5dc1b00f"
      },
      "outputs": [],
      "source": [
        "# preprocess sentences\n",
        "for i, row in euph_corpus.iterrows():\n",
        "    text = euph_corpus.loc[i, 'sentence']\n",
        "    euph_corpus.loc[i, 'sentence'] = preprocess(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "777f7576-aac1-4d98-8499-b7253e927314",
      "metadata": {
        "id": "777f7576-aac1-4d98-8499-b7253e927314",
        "outputId": "f9cc0053-e32a-4f4a-968d-8aad21fdb614"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keyword</th>\n",
              "      <th>edited_text</th>\n",
              "      <th>is_euph</th>\n",
              "      <th>category</th>\n",
              "      <th>type</th>\n",
              "      <th>euph_status</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tinkle</td>\n",
              "      <td>We're just getting back what was TAKEN from us...</td>\n",
              "      <td>1</td>\n",
              "      <td>bodily functions</td>\n",
              "      <td>tinkle</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>we're just getting back what was taken from us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tinkle</td>\n",
              "      <td>I think AB390 will pass next year now that the...</td>\n",
              "      <td>1</td>\n",
              "      <td>bodily functions</td>\n",
              "      <td>tinkle</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>i think ab390 will pass next year now that the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>undocumented immigrants</td>\n",
              "      <td>Singled Out Think Like a Man, the new movie ba...</td>\n",
              "      <td>1</td>\n",
              "      <td>politics</td>\n",
              "      <td>undocumented immigrant</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>anything but secure a federal program designed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>undocumented immigrants</td>\n",
              "      <td>Not to be outdone, Sen. Rand Paul (R-Ky. ), so...</td>\n",
              "      <td>1</td>\n",
              "      <td>politics</td>\n",
              "      <td>undocumented immigrant</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>in a post-election interview with politico pau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>undocumented immigrants</td>\n",
              "      <td>The law has also galvanized the growing immigr...</td>\n",
              "      <td>1</td>\n",
              "      <td>politics</td>\n",
              "      <td>undocumented immigrant</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>aside from undocumented immigrants the america...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   keyword                                        edited_text  \\\n",
              "0                   tinkle  We're just getting back what was TAKEN from us...   \n",
              "1                   tinkle  I think AB390 will pass next year now that the...   \n",
              "2  undocumented immigrants  Singled Out Think Like a Man, the new movie ba...   \n",
              "3  undocumented immigrants  Not to be outdone, Sen. Rand Paul (R-Ky. ), so...   \n",
              "4  undocumented immigrants  The law has also galvanized the growing immigr...   \n",
              "\n",
              "   is_euph          category                    type  euph_status  \\\n",
              "0        1  bodily functions                  tinkle  always_euph   \n",
              "1        1  bodily functions                  tinkle  always_euph   \n",
              "2        1          politics  undocumented immigrant  always_euph   \n",
              "3        1          politics  undocumented immigrant  always_euph   \n",
              "4        1          politics  undocumented immigrant  always_euph   \n",
              "\n",
              "                                            sentence  \n",
              "0  we're just getting back what was taken from us...  \n",
              "1  i think ab390 will pass next year now that the...  \n",
              "2  anything but secure a federal program designed...  \n",
              "3  in a post-election interview with politico pau...  \n",
              "4  aside from undocumented immigrants the america...  "
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "euph_corpus.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3882de19-65f2-4341-8b09-a02d761072ef",
      "metadata": {
        "id": "3882de19-65f2-4341-8b09-a02d761072ef"
      },
      "source": [
        "# Autophrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9386b51a-7a1d-4ecf-bd63-529505928fe0",
      "metadata": {
        "id": "9386b51a-7a1d-4ecf-bd63-529505928fe0"
      },
      "outputs": [],
      "source": [
        "# create phrase column with found phrases through AutoPhrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a69731fb-5cb7-4236-af9a-0a80a17bc007",
      "metadata": {
        "id": "a69731fb-5cb7-4236-af9a-0a80a17bc007"
      },
      "outputs": [],
      "source": [
        "multidata = pd.read_csv('/home/gavidiam1/Downloads/euphcorpus_AutoPhrase_multi-words.txt', sep='\\t', header=None)\n",
        "multidata.columns = ['score','mwe']\n",
        "multidata['mwe'] = multidata['mwe'].str.strip()\n",
        "#multidata['mwe2'] = multidata['mwe'].str.replace(r' ','_')\n",
        "multi = multidata['mwe'].to_list()\n",
        "#multi2 = multidata['mwe2'].to_list()\n",
        "\n",
        "singledata = pd.read_csv('/home/gavidiam1/Downloads/euphcorpus_AutoPhrase_single-word.txt', sep='\\t',header=None)\n",
        "singledata.columns = ['score', 'phrase']\n",
        "singledata['phrase'] =singledata['phrase'].str.strip()\n",
        "singledata['phrase'] = singledata['phrase'].astype(str)\n",
        "single = singledata['phrase'].to_list()\n",
        "\n",
        "#combine all\n",
        "autophrases = (multi+single)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae27e8ac-faac-4926-9863-b7ed9c11db2d",
      "metadata": {
        "id": "ae27e8ac-faac-4926-9863-b7ed9c11db2d"
      },
      "source": [
        "# PhraseMatcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ba395dd-fb88-4b2f-be2c-faecd793e2f4",
      "metadata": {
        "id": "7ba395dd-fb88-4b2f-be2c-faecd793e2f4"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f76e48c0-e48c-4ce2-b8b4-c35e832fe2a4",
      "metadata": {
        "id": "f76e48c0-e48c-4ce2-b8b4-c35e832fe2a4",
        "outputId": "7af99f3d-71de-4ec2-f1b0-4dd2e5b7cbbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download('en_core_web_md')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efd5751c-8868-4b41-92ed-495bde6b8329",
      "metadata": {
        "id": "efd5751c-8868-4b41-92ed-495bde6b8329",
        "outputId": "961ba687-bc12-420f-e879-36200d413e5a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1965/1965 [00:26<00:00, 74.80it/s]\n"
          ]
        }
      ],
      "source": [
        "#Find Autophrase phrases in text\n",
        "from tqdm import tqdm\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "matcher = PhraseMatcher(nlp.vocab,attr=\"LOWER\")\n",
        "terms = autophrases\n",
        "patterns = [nlp.make_doc(text) for text in terms]\n",
        "matcher.add(\"TerminologyList\", patterns)\n",
        "\n",
        "with tqdm(total=len(euph_corpus)) as pbar: # this loop is for the progress bar\n",
        "    for index, row in euph_corpus.iterrows():\n",
        "        d = []\n",
        "        doc = nlp(row['sentence'])\n",
        "        matches = matcher(doc)\n",
        "        for match_id, start, end in matches:\n",
        "            span = doc[start : end]  # get the matched slice of the doc\n",
        "            d.append(span.text)\n",
        "            euph_corpus.at[index, 'autophrase'] = d\n",
        "        pbar.update(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18b367e0-c430-4bf0-a4f2-84317f718e8c",
      "metadata": {
        "id": "18b367e0-c430-4bf0-a4f2-84317f718e8c",
        "outputId": "6ec20810-680f-4633-bc18-71134cb1269a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keyword</th>\n",
              "      <th>edited_text</th>\n",
              "      <th>is_euph</th>\n",
              "      <th>category</th>\n",
              "      <th>type</th>\n",
              "      <th>euph_status</th>\n",
              "      <th>sentence</th>\n",
              "      <th>autophrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tinkle</td>\n",
              "      <td>We're just getting back what was TAKEN from us...</td>\n",
              "      <td>1</td>\n",
              "      <td>bodily functions</td>\n",
              "      <td>tinkle</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>we're just getting back what was taken from us...</td>\n",
              "      <td>[we, we're, just, getting, back, what, was, ta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tinkle</td>\n",
              "      <td>I think AB390 will pass next year now that the...</td>\n",
              "      <td>1</td>\n",
              "      <td>bodily functions</td>\n",
              "      <td>tinkle</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>i think ab390 will pass next year now that the...</td>\n",
              "      <td>[i, think, will, pass, next, year, now, that, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>undocumented immigrants</td>\n",
              "      <td>Singled Out Think Like a Man, the new movie ba...</td>\n",
              "      <td>1</td>\n",
              "      <td>politics</td>\n",
              "      <td>undocumented immigrant</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>anything but secure a federal program designed...</td>\n",
              "      <td>[anything, but, a, program, to, undocumented, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>undocumented immigrants</td>\n",
              "      <td>Not to be outdone, Sen. Rand Paul (R-Ky. ), so...</td>\n",
              "      <td>1</td>\n",
              "      <td>politics</td>\n",
              "      <td>undocumented immigrant</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>in a post-election interview with politico pau...</td>\n",
              "      <td>[in, a, election, with, said, he, wants, to, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>undocumented immigrants</td>\n",
              "      <td>The law has also galvanized the growing immigr...</td>\n",
              "      <td>1</td>\n",
              "      <td>politics</td>\n",
              "      <td>undocumented immigrant</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>aside from undocumented immigrants the america...</td>\n",
              "      <td>[from, undocumented, undocumented immigrants, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   keyword                                        edited_text  \\\n",
              "0                   tinkle  We're just getting back what was TAKEN from us...   \n",
              "1                   tinkle  I think AB390 will pass next year now that the...   \n",
              "2  undocumented immigrants  Singled Out Think Like a Man, the new movie ba...   \n",
              "3  undocumented immigrants  Not to be outdone, Sen. Rand Paul (R-Ky. ), so...   \n",
              "4  undocumented immigrants  The law has also galvanized the growing immigr...   \n",
              "\n",
              "   is_euph          category                    type  euph_status  \\\n",
              "0        1  bodily functions                  tinkle  always_euph   \n",
              "1        1  bodily functions                  tinkle  always_euph   \n",
              "2        1          politics  undocumented immigrant  always_euph   \n",
              "3        1          politics  undocumented immigrant  always_euph   \n",
              "4        1          politics  undocumented immigrant  always_euph   \n",
              "\n",
              "                                            sentence  \\\n",
              "0  we're just getting back what was taken from us...   \n",
              "1  i think ab390 will pass next year now that the...   \n",
              "2  anything but secure a federal program designed...   \n",
              "3  in a post-election interview with politico pau...   \n",
              "4  aside from undocumented immigrants the america...   \n",
              "\n",
              "                                          autophrase  \n",
              "0  [we, we're, just, getting, back, what, was, ta...  \n",
              "1  [i, think, will, pass, next, year, now, that, ...  \n",
              "2  [anything, but, a, program, to, undocumented, ...  \n",
              "3  [in, a, election, with, said, he, wants, to, t...  \n",
              "4  [from, undocumented, undocumented immigrants, ...  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "euph_corpus.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ecb1fa-7137-458c-a5e8-d13e67b221f5",
      "metadata": {
        "id": "c2ecb1fa-7137-458c-a5e8-d13e67b221f5",
        "outputId": "beeac66a-948f-45eb-9684-bf5e05775802"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['from',\n",
              " 'undocumented',\n",
              " 'undocumented immigrants',\n",
              " 'immigrants',\n",
              " 'the',\n",
              " 'the american',\n",
              " 'american',\n",
              " 'citizens',\n",
              " 'who',\n",
              " 'make',\n",
              " 'up',\n",
              " 'what',\n",
              " 'i',\n",
              " 'call',\n",
              " 'the',\n",
              " 'of',\n",
              " 'people',\n",
              " 'like',\n",
              " 'who',\n",
              " 'for',\n",
              " 'immigration',\n",
              " 'even',\n",
              " 'though',\n",
              " 'her',\n",
              " 'a',\n",
              " 'in',\n",
              " 'was',\n",
              " 'killed',\n",
              " 'by',\n",
              " 'an',\n",
              " 'undocumented',\n",
              " 'are',\n",
              " 'up',\n",
              " 'and',\n",
              " 'out']"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "euph_corpus['autophrase'][4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7270a304-65f2-47a9-abac-8c66d4958cb1",
      "metadata": {
        "id": "7270a304-65f2-47a9-abac-8c66d4958cb1"
      },
      "outputs": [],
      "source": [
        "for i, row in euph_corpus.iterrows():\n",
        "    text = euph_corpus.loc[i, 'autophrase']\n",
        "    text = [s.replace(' ', '_') for s in text]\n",
        "    euph_corpus.at[i, 'autophrase'] = text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f40543ff-c483-4ce9-867f-2af556ddc4be",
      "metadata": {
        "id": "f40543ff-c483-4ce9-867f-2af556ddc4be",
        "outputId": "36be377e-ee1f-47f4-af5c-a3375573c872"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keyword</th>\n",
              "      <th>edited_text</th>\n",
              "      <th>is_euph</th>\n",
              "      <th>category</th>\n",
              "      <th>type</th>\n",
              "      <th>euph_status</th>\n",
              "      <th>sentence</th>\n",
              "      <th>autophrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tinkle</td>\n",
              "      <td>We're just getting back what was TAKEN from us...</td>\n",
              "      <td>1</td>\n",
              "      <td>bodily functions</td>\n",
              "      <td>tinkle</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>we're just getting back what was taken from us...</td>\n",
              "      <td>[we, we're, just, getting, back, what, was, ta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tinkle</td>\n",
              "      <td>I think AB390 will pass next year now that the...</td>\n",
              "      <td>1</td>\n",
              "      <td>bodily functions</td>\n",
              "      <td>tinkle</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>i think ab390 will pass next year now that the...</td>\n",
              "      <td>[i, think, will, pass, next, year, now, that, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>undocumented immigrants</td>\n",
              "      <td>Singled Out Think Like a Man, the new movie ba...</td>\n",
              "      <td>1</td>\n",
              "      <td>politics</td>\n",
              "      <td>undocumented immigrant</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>anything but secure a federal program designed...</td>\n",
              "      <td>[anything, but, a, program, to, undocumented, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>undocumented immigrants</td>\n",
              "      <td>Not to be outdone, Sen. Rand Paul (R-Ky. ), so...</td>\n",
              "      <td>1</td>\n",
              "      <td>politics</td>\n",
              "      <td>undocumented immigrant</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>in a post-election interview with politico pau...</td>\n",
              "      <td>[in, a, election, with, said, he, wants, to, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>undocumented immigrants</td>\n",
              "      <td>The law has also galvanized the growing immigr...</td>\n",
              "      <td>1</td>\n",
              "      <td>politics</td>\n",
              "      <td>undocumented immigrant</td>\n",
              "      <td>always_euph</td>\n",
              "      <td>aside from undocumented immigrants the america...</td>\n",
              "      <td>[from, undocumented, undocumented_immigrants, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1960</th>\n",
              "      <td>sleep with</td>\n",
              "      <td>There were other photos she wanted me to see: ...</td>\n",
              "      <td>0</td>\n",
              "      <td>sexual activity</td>\n",
              "      <td>sleep with</td>\n",
              "      <td>sometimes_euph</td>\n",
              "      <td>there were other photos she wanted me to see b...</td>\n",
              "      <td>[there, were, other, she, wanted, me, to, to_s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1961</th>\n",
              "      <td>sleep with</td>\n",
              "      <td>I am relieved to see two pup tents marked STAF...</td>\n",
              "      <td>0</td>\n",
              "      <td>sexual activity</td>\n",
              "      <td>sleep with</td>\n",
              "      <td>sometimes_euph</td>\n",
              "      <td>thank god i don't have to sleep with ace wands</td>\n",
              "      <td>[god, i, do, don't, have, to, to_sleep, sleep,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1962</th>\n",
              "      <td>sleep around</td>\n",
              "      <td>Nothing serious, just long nights of me hackin...</td>\n",
              "      <td>0</td>\n",
              "      <td>sexual activity</td>\n",
              "      <td>sleep around</td>\n",
              "      <td>sometimes_euph</td>\n",
              "      <td>with all my caterwauling it's a wonder anyone ...</td>\n",
              "      <td>[with, all, my, it, it's, a, anyone, gets, any...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1963</th>\n",
              "      <td>with child</td>\n",
              "      <td>sounds more like Jonestown. They cant leave @ ...</td>\n",
              "      <td>0</td>\n",
              "      <td>physical/mental attributes</td>\n",
              "      <td>with child</td>\n",
              "      <td>sometimes_euph</td>\n",
              "      <td>they cant leave best advice i can give them is...</td>\n",
              "      <td>[they, leave, best, i, can, give, them, is, if...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1964</th>\n",
              "      <td>with child</td>\n",
              "      <td>Nickname of a girl named Diana. 41. What you d...</td>\n",
              "      <td>0</td>\n",
              "      <td>physical/mental attributes</td>\n",
              "      <td>with child</td>\n",
              "      <td>sometimes_euph</td>\n",
              "      <td>what you do with child life 42</td>\n",
              "      <td>[what, you, do, with, child, life]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1965 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      keyword  \\\n",
              "0                      tinkle   \n",
              "1                      tinkle   \n",
              "2     undocumented immigrants   \n",
              "3     undocumented immigrants   \n",
              "4     undocumented immigrants   \n",
              "...                       ...   \n",
              "1960               sleep with   \n",
              "1961               sleep with   \n",
              "1962             sleep around   \n",
              "1963               with child   \n",
              "1964               with child   \n",
              "\n",
              "                                            edited_text  is_euph  \\\n",
              "0     We're just getting back what was TAKEN from us...        1   \n",
              "1     I think AB390 will pass next year now that the...        1   \n",
              "2     Singled Out Think Like a Man, the new movie ba...        1   \n",
              "3     Not to be outdone, Sen. Rand Paul (R-Ky. ), so...        1   \n",
              "4     The law has also galvanized the growing immigr...        1   \n",
              "...                                                 ...      ...   \n",
              "1960  There were other photos she wanted me to see: ...        0   \n",
              "1961  I am relieved to see two pup tents marked STAF...        0   \n",
              "1962  Nothing serious, just long nights of me hackin...        0   \n",
              "1963  sounds more like Jonestown. They cant leave @ ...        0   \n",
              "1964  Nickname of a girl named Diana. 41. What you d...        0   \n",
              "\n",
              "                        category                    type     euph_status  \\\n",
              "0               bodily functions                  tinkle     always_euph   \n",
              "1               bodily functions                  tinkle     always_euph   \n",
              "2                       politics  undocumented immigrant     always_euph   \n",
              "3                       politics  undocumented immigrant     always_euph   \n",
              "4                       politics  undocumented immigrant     always_euph   \n",
              "...                          ...                     ...             ...   \n",
              "1960             sexual activity              sleep with  sometimes_euph   \n",
              "1961             sexual activity              sleep with  sometimes_euph   \n",
              "1962             sexual activity            sleep around  sometimes_euph   \n",
              "1963  physical/mental attributes              with child  sometimes_euph   \n",
              "1964  physical/mental attributes              with child  sometimes_euph   \n",
              "\n",
              "                                               sentence  \\\n",
              "0     we're just getting back what was taken from us...   \n",
              "1     i think ab390 will pass next year now that the...   \n",
              "2     anything but secure a federal program designed...   \n",
              "3     in a post-election interview with politico pau...   \n",
              "4     aside from undocumented immigrants the america...   \n",
              "...                                                 ...   \n",
              "1960  there were other photos she wanted me to see b...   \n",
              "1961     thank god i don't have to sleep with ace wands   \n",
              "1962  with all my caterwauling it's a wonder anyone ...   \n",
              "1963  they cant leave best advice i can give them is...   \n",
              "1964                     what you do with child life 42   \n",
              "\n",
              "                                             autophrase  \n",
              "0     [we, we're, just, getting, back, what, was, ta...  \n",
              "1     [i, think, will, pass, next, year, now, that, ...  \n",
              "2     [anything, but, a, program, to, undocumented, ...  \n",
              "3     [in, a, election, with, said, he, wants, to, t...  \n",
              "4     [from, undocumented, undocumented_immigrants, ...  \n",
              "...                                                 ...  \n",
              "1960  [there, were, other, she, wanted, me, to, to_s...  \n",
              "1961  [god, i, do, don't, have, to, to_sleep, sleep,...  \n",
              "1962  [with, all, my, it, it's, a, anyone, gets, any...  \n",
              "1963  [they, leave, best, i, can, give, them, is, if...  \n",
              "1964                 [what, you, do, with, child, life]  \n",
              "\n",
              "[1965 rows x 8 columns]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "euph_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03601376-910a-45cc-968d-abf8f39eab5f",
      "metadata": {
        "id": "03601376-910a-45cc-968d-abf8f39eab5f"
      },
      "outputs": [],
      "source": [
        "euph_corpus.to_csv('euph_corpus_segmented.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "158b5e12-c61b-4888-bd80-31106155ba23",
      "metadata": {
        "id": "158b5e12-c61b-4888-bd80-31106155ba23"
      },
      "source": [
        "## Similarity measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afbdd8dd-b41b-4c07-b626-f9acb83bca8d",
      "metadata": {
        "id": "afbdd8dd-b41b-4c07-b626-f9acb83bca8d"
      },
      "outputs": [],
      "source": [
        "def sum_similarity(phrase, topic_list):\n",
        "    score = 0\n",
        "    for topic in topic_list:\n",
        "        try:\n",
        "            similarity = model.wv.similarity(phrase, topic)\n",
        "            if (similarity > 0):\n",
        "                score += similarity\n",
        "        except:\n",
        "            score += 0\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf07558-f061-43ca-bc49-5177a821f8ee",
      "metadata": {
        "id": "1cf07558-f061-43ca-bc49-5177a821f8ee"
      },
      "outputs": [],
      "source": [
        "# define topic list and stopwords\n",
        "topic_list = ['politics', 'death', 'kill', 'crime',\n",
        "               'drugs', 'alcohol', 'fat', 'old', 'poor', 'cheap',\n",
        "               'sex', 'sexual',\n",
        "               'employment', 'job', 'disability', 'disabled', \n",
        "               'accident', 'pregnant', 'poop', 'sickness', 'race', 'racial', 'vomit'\n",
        "              ]\n",
        "\n",
        "stopwords = []\n",
        "#['the', 'a', 'to', 'him', 'her', 'them', 'me', 'you', 'of', 'with']\n",
        "\n",
        "with open('/home/gavidiam1/stopwords.txt','rb') as f:\n",
        "    content = f.read()\n",
        "    content = content.split(b'\\r\\n')\n",
        "    for line in content:\n",
        "        stopwords.append(line.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63053f31-e64f-4750-9681-42352b56f61b",
      "metadata": {
        "id": "63053f31-e64f-4750-9681-42352b56f61b"
      },
      "source": [
        "# Train Glowbe with word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e814ef43-7118-451f-b934-9568f5f47e3a",
      "metadata": {
        "id": "e814ef43-7118-451f-b934-9568f5f47e3a"
      },
      "outputs": [],
      "source": [
        "# define model and train on new data\n",
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec.load(\"wv_model_7\")\n",
        "# train model on input data \n",
        "model.train(data, total_examples=len(data), epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7b12450-faa5-4147-9441-c176e4a71c43",
      "metadata": {
        "id": "f7b12450-faa5-4147-9441-c176e4a71c43",
        "outputId": "1d86b498-cae4-4d42-c812-447a3c620c78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Retained the euphemism in 1769 out of 1965 sentences'\n",
            "'45349 quality phrases retained overall'\n",
            "'Filtered 1646 non-keywords out'\n",
            "(\"EXACT SUCCESSES: ['tinkle', 'undocumented immigrants', 'undocumented \"\n",
            " \"immigrant', 'venereal diseases', 'venereal disease', 'sex workers', 'sex \"\n",
            " \"worker', 'mentally disabled', 'correctional facilities', 'correctional \"\n",
            " \"facility', 'freedom fighters', 'freedom fighter', 'detainees', 'detainee', \"\n",
            " \"'comfort women', 'psychiatric hospital', 'ethnic cleansing', 'ethnically \"\n",
            " \"cleansed', 'enhanced interrogation techniques', 'mistruths', 'elderly', \"\n",
            " \"'armed conflict', 'drinking problem', 'deceased', 'income inequality', 'rear \"\n",
            " \"end', 'lavatory', 'inner city', 'developing country', 'developed country', \"\n",
            " \"'substance abuse', 'global south', 'underprivileged', 'inebriated', \"\n",
            " \"'homemaker', 'capital punishment', 'indigent', 'detention camp', 'pass gas', \"\n",
            " \"'dearly departed', 'pregnancy termination', 'senior citizen', 'senior \"\n",
            " \"citizens', 'substance abuser', 'substance abusers', 'undocumented workers', \"\n",
            " \"'sanitation workers', 'full figured', 'latrine', 'physically challenged', \"\n",
            " \"'developmentally disabled', 'custodians', 'same sex', 'hearing impaired', \"\n",
            " \"'people of color', 'adult beverages', 'broken home', 'fatality', \"\n",
            " \"'fatalities', 'targeted killings', 'targeted killing', 'less fortunate', \"\n",
            " \"'advanced age', 'mentally challenged', 'droppings', 'portly', 'negative cash \"\n",
            " \"flow', 'golden years', 'making love', 'running behind', 'let go of', \"\n",
            " \"'letting someone go', 'perished', 'perish', 'passing on', 'pass on', 'passed \"\n",
            " \"away', 'passing away', 'pass away', 'neutralize', 'neutralized', \"\n",
            " \"'overweight', 'aging', 'chest', 'demise', 'slim', 'dismissed', 'sober', \"\n",
            " \"'collateral damage', 'deprived', 'plump', 'stout', 'disabled', 'special \"\n",
            " \"needs', 'disadvantaged', 'underdeveloped', 'invalid', 'mixed up', 'well \"\n",
            " \"off', 'economical', 'intoxicated', 'regime change', 'exterminate', 'a \"\n",
            " \"certain age', 'custodian', 'put to sleep', 'long sleep', 'downsize', \"\n",
            " \"'expecting', 'lay off', 'laid off', 'laying off', 'to go to heaven', 'went \"\n",
            " \"to heaven', 'accident', 'gluteus maximus', 'oldest profession', 'late', \"\n",
            " \"'experienced', 'outspoken', 'wealthy', 'troubled', 'seeing someone', \"\n",
            " \"'seasoned', 'slept with', 'sleep with', 'weed']\")\n",
            "\"PARTIAL SUCCESSES: ['sleep around', 'with child']\"\n",
            "(\"FAILURES: ['lost my lunch', 'lose your lunch', 'lose their lunch', \"\n",
            " \"'mistruth', 'pro-life', 'birds and the bees', 'differently-abled', \"\n",
            " \"'economical with the truth', 'terminating a pregnancy', 'substance abusers', \"\n",
            " \"'pre-owned', 'sanitation worker', 'street person', 'plus-sized', \"\n",
            " \"'able-bodied', 'under the weather', 'pro-choice', 'low-income', 'time of the \"\n",
            " \"month', 'made love', 'make love', 'let him go', 'same-sex', 'go all the \"\n",
            " \"way', 'over the hill', 'intoxicated', 'between jobs', 'getting clean', \"\n",
            " \"'outlived her usefulness', 'outlived their usefulness', 'got clean', 'with \"\n",
            " \"child', 'let them go', 'let us go', 'let her go', 'outlived his usefulness', \"\n",
            " \"'seeing each other', 'sleep around']\")\n",
            "'FALSE NEGATIVES of TOPIC FILTERING: []'\n"
          ]
        }
      ],
      "source": [
        "THRESHOLD = 1.5\n",
        "score = 0\n",
        "\n",
        "successes = []\n",
        "partial_successes = []\n",
        "failures = []\n",
        "topically_filtered_euphs = []\n",
        "quality_phrase_count = 0\n",
        "filtered = []\n",
        "\n",
        "euph_corpus['quality_phrases'] = \"\"\n",
        "\n",
        "for i, row in euph_corpus.iterrows():\n",
        "    text = euph_corpus.loc[i, 'sentence']\n",
        "    phrases = euph_corpus.loc[i, 'autophrase']\n",
        "    euph = euph_corpus.loc[i, 'keyword']\n",
        "    quality_phrases = []\n",
        "    for phrase in phrases:\n",
        "        if (phrase in stopwords):\n",
        "            continue\n",
        "        similarity = sum_similarity(phrase, topic_list)\n",
        "        if (similarity > THRESHOLD and phrase not in quality_phrases):\n",
        "            quality_phrases.append(phrase)\n",
        "        elif (similarity < THRESHOLD and euph == re.sub(r'_', ' ', phrase)):\n",
        "            if euph not in topically_filtered_euphs:\n",
        "                topically_filtered_euphs.append(euph)\n",
        "        else:\n",
        "            filtered.append(phrase)\n",
        "    # add the quality phrases to the column\n",
        "    euph_corpus.at[i, 'quality_phrases'] = quality_phrases\n",
        "    \n",
        "    # now check if the euph in the sentence is retained in the list of quality phrases\n",
        "    quality_phrases = [re.sub(r'_', ' ', p) for p in quality_phrases]\n",
        "    quality_phrase_count += len(quality_phrases)\n",
        "    \n",
        "    if euph in quality_phrases:\n",
        "        score += 1\n",
        "        if euph not in successes:\n",
        "            successes.append(euph)\n",
        "    else:\n",
        "        partial_success = False\n",
        "        for p in quality_phrases: # check if phrase output contains euphemism\n",
        "            if euph in p:\n",
        "                score += 1\n",
        "                if euph not in partial_successes:\n",
        "                    partial_successes.append(euph)\n",
        "                    partial_success = True\n",
        "                    break\n",
        "        if (partial_success == False): \n",
        "            if euph not in failures:\n",
        "                failures.append(euph)\n",
        "\n",
        "            # check failures for a particular phrase\n",
        "            # if (euph == \"ethnic cleansing\"):\n",
        "            #     print(\"TEXT: {}\".format(text))\n",
        "            #     print(\"PHRASES: {}\".format(phrases))\n",
        "            #     print(\"QUALITY PHRASES: {}\".format(quality_phrases))\n",
        "            #     print()\n",
        "\n",
        "print(\"Retained the euphemism in {} out of {} sentences\".format(score, len(euph_corpus)))\n",
        "print(\"{} quality phrases retained overall\".format(quality_phrase_count))\n",
        "print(\"Filtered {} non-keywords out\".format(len(filtered)))\n",
        "#print()\n",
        "print(\"EXACT SUCCESSES: {}\".format(successes))\n",
        "#print()\n",
        "print(\"PARTIAL SUCCESSES: {}\".format(partial_successes))\n",
        "#print()\n",
        "print(\"FAILURES: {}\".format(failures))\n",
        "#print()\n",
        "print(\"FALSE NEGATIVES of TOPIC FILTERING: {}\".format(topically_filtered_euphs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "206fc8dd-3b8b-428f-ae13-85543da049c1",
      "metadata": {
        "id": "206fc8dd-3b8b-428f-ae13-85543da049c1"
      },
      "source": [
        "# Sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5035a67-0ead-4764-9cd7-76d73929b676",
      "metadata": {
        "id": "c5035a67-0ead-4764-9cd7-76d73929b676"
      },
      "source": [
        "## roBERTa Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dde9c336-8f90-410c-9ca8-b73ecad1af23",
      "metadata": {
        "id": "dde9c336-8f90-410c-9ca8-b73ecad1af23"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import csv\n",
        "import urllib.request\n",
        "\n",
        "def load_roberta_sentiment():\n",
        "    # Tasks:\n",
        "    # emoji, emotion, hate, irony, offensive, sentiment\n",
        "    # stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
        "\n",
        "    task='sentiment'\n",
        "    MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "    # download label mapping\n",
        "    labels=[]\n",
        "    mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
        "    with urllib.request.urlopen(mapping_link) as f:\n",
        "        html = f.read().decode('utf-8').split(\"\\n\")\n",
        "        csvreader = csv.reader(html, delimiter='\\t')\n",
        "    labels = [row[1] for row in csvreader if len(row) > 1]\n",
        "\n",
        "    # pretrained\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "    model.save_pretrained(MODEL)\n",
        "    tokenizer.save_pretrained(MODEL)\n",
        "    \n",
        "    return labels, model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c58d14-7fe4-4f7b-944a-892c279d7377",
      "metadata": {
        "id": "27c58d14-7fe4-4f7b-944a-892c279d7377"
      },
      "source": [
        "## Offensive sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6d5abc-7c7d-4de8-a01e-58df2f5f272e",
      "metadata": {
        "id": "4d6d5abc-7c7d-4de8-a01e-58df2f5f272e"
      },
      "outputs": [],
      "source": [
        "def load_roberta_offensive():\n",
        "    task='offensive'\n",
        "    MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "    # download label mapping\n",
        "    labels=[]\n",
        "    mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
        "    with urllib.request.urlopen(mapping_link) as f:\n",
        "        html = f.read().decode('utf-8').split(\"\\n\")\n",
        "        csvreader = csv.reader(html, delimiter='\\t')\n",
        "    labels = [row[1] for row in csvreader if len(row) > 1]\n",
        "\n",
        "    # PT\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "    model.save_pretrained(MODEL)\n",
        "    tokenizer.save_pretrained(MODEL)\n",
        "    \n",
        "    return labels, model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "317bd311-2414-4d36-9227-9be870b9d2f1",
      "metadata": {
        "id": "317bd311-2414-4d36-9227-9be870b9d2f1"
      },
      "outputs": [],
      "source": [
        "def get_sentiment(s, labels, model, tokenizer):\n",
        "    encoded_input = tokenizer(s, return_tensors='pt')\n",
        "    output = model(**encoded_input)\n",
        "    scores = output[0][0].detach().numpy()\n",
        "    scores = softmax(scores)\n",
        "    # ranking = np.argsort(scores)\n",
        "    # ranking = ranking[::-1]\n",
        "    # for i in range(scores.shape[0]):\n",
        "    #     l = labels[ranking[i]]\n",
        "    #     s = scores[ranking[i]]\n",
        "        # print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
        "    return scores\n",
        "\n",
        "def get_offensive(s, labels, model, tokenizer):\n",
        "    encoded_input = tokenizer(s, return_tensors='pt')\n",
        "    output = model(**encoded_input)\n",
        "    scores = output[0][0].detach().numpy()\n",
        "    scores = softmax(scores)\n",
        "    # ranking = np.argsort(scores)\n",
        "    # ranking = ranking[::-1]\n",
        "    # for i in range(0, 2):\n",
        "    #     l = labels[ranking[i]]\n",
        "    #     s = scores[ranking[i]]\n",
        "        # print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b543a14d-1859-405f-be19-1ce7aa61f0b6",
      "metadata": {
        "id": "b543a14d-1859-405f-be19-1ce7aa61f0b6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "needs functions load_roberta_sentiment(), load_roberta_offensive(), get_sentiment() and get_offensive()\n",
        "'''\n",
        "def get_top_euph_candidates(text, phrases, num_paraphrases, wv_model, sentiment_pack, offensive_pack, show_stats=False):\n",
        "    \n",
        "    sentiment_labels, sentiment_model, sentiment_tokenizer = sentiment_pack[0], sentiment_pack[1], sentiment_pack[2]\n",
        "    offensive_labels, offensive_model, offensive_tokenizer = offensive_pack[0], offensive_pack[1], offensive_pack[2]\n",
        "    \n",
        "    orig_scores = list(get_sentiment(text, sentiment_labels, sentiment_model, sentiment_tokenizer))\n",
        "    orig_scores = orig_scores + list(get_offensive(text, offensive_labels, offensive_model, offensive_tokenizer))\n",
        "    if show_stats == True: print('SENTIMENT OF ORIGINAL SENTENCE: {}'.format(orig_scores))\n",
        "    phrase_scores = []\n",
        "\n",
        "    for q in phrases:\n",
        "        paraphrases = []\n",
        "        if show_stats == True: print('\\n'+q)\n",
        "        paraphrases = wv_model.wv.most_similar(q, topn = num_paraphrases) # can swap out\n",
        "        \n",
        "        # print(paraphrases)\n",
        "        \n",
        "        # various sentiment statistics\n",
        "        sentiment_shift = [0, 0, 0, 0, 0]\n",
        "        max_inc = [0, 0, 0, 0, 0]\n",
        "        max_inc_para = [\"\", \"\", \"\", \"\", \"\"]\n",
        "        tot_neg_inc = 0\n",
        "        tot_neu_inc = 0\n",
        "        tot_off_inc = 0\n",
        "        tot_noff_inc = 0\n",
        "        \n",
        "        for p in paraphrases:\n",
        "            p_string = re.sub(r'_', ' ', p[0]) # the underscores are removed for sentiment computation - experiment?\n",
        "            q_string = re.sub(r'_', ' ', q)\n",
        "            # replacement\n",
        "            pattern = re.compile(r'\\b'+q_string+r'\\b', re.IGNORECASE)\n",
        "            new_sentence = pattern.sub(p_string, text)\n",
        "            # at this point, we could check the integrity of the paraphrase\n",
        "\n",
        "            # get the sentiment/offensive scores for this paraphrase\n",
        "            scores = list(get_sentiment(new_sentence, sentiment_labels, sentiment_model, sentiment_tokenizer))\n",
        "            scores = scores + list(get_offensive(new_sentence, offensive_labels, offensive_model, offensive_tokenizer))\n",
        "\n",
        "            # update the quality phrase's sentiment statistics with the sentiment shifts from this paraphrase\n",
        "            shifts = [0, 0, 0, 0, 0]\n",
        "            for i in range(0, len(scores)):\n",
        "                shifts[i] = scores[i] - orig_scores[i]\n",
        "                sentiment_shift[i] += shifts[i]\n",
        "                if (shifts[i] > max_inc[i]):\n",
        "                    max_inc[i] = shifts[i]\n",
        "                    max_inc_para[i] = p_string\n",
        "\n",
        "            # update the relevant scores for detection\n",
        "            if (shifts[0] > 0):\n",
        "                tot_neg_inc += shifts[0]\n",
        "            if (shifts[1] > 0):\n",
        "                tot_neu_inc += shifts[1]\n",
        "            if (shifts[3] > 0):\n",
        "                tot_noff_inc += shifts[3]\n",
        "            if (shifts[4] > 0):\n",
        "                tot_off_inc += shifts[4]\n",
        "        \n",
        "        for val in sentiment_shift:\n",
        "            val /= num_paraphrases\n",
        "        if (show_stats == True):\n",
        "            print(\"AVERAGE SENTIMENT SHIFTS: {}\".format(sentiment_shift))\n",
        "            print(\"MAX INCREASE FROM A PHRASE: {}\".format(max_inc))\n",
        "            print(\"PHRASES THAT CAUSED EACH ^: {}\".format(max_inc_para))\n",
        "            print(\"TOTAL NEGATIVE INCREASE: {}\".format(tot_neg_inc))\n",
        "            print(\"TOTAL NEUTRAL INCREASE: {}\".format(tot_neu_inc))\n",
        "            print(\"TOTAL NEUTRAL INCREASE: {}\".format(tot_noff_inc))\n",
        "            print(\"TOTAL OFFENSIVE INCREASE: {}\".format(tot_off_inc))\n",
        "\n",
        "        phrase_scores.append((q_string, tot_neg_inc + tot_neu_inc + 2*(tot_noff_inc + tot_off_inc)))\n",
        "\n",
        "    phrase_scores = list(sorted(phrase_scores, key=lambda x: x[1], reverse=True))\n",
        "    return phrase_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c613b09d-262b-4d65-9093-8875d9119426",
      "metadata": {
        "id": "c613b09d-262b-4d65-9093-8875d9119426"
      },
      "outputs": [],
      "source": [
        "# load the models\n",
        "sentiment_labels, sentiment_model, sentiment_tokenizer = load_roberta_sentiment()\n",
        "offensive_labels, offensive_model, offensive_tokenizer = load_roberta_offensive()\n",
        "\n",
        "sentiment_pack = [sentiment_labels, sentiment_model, sentiment_tokenizer]\n",
        "offensive_pack = [offensive_labels, offensive_model, offensive_tokenizer]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4336fd1f-c330-4073-9167-12d662e9fb6a",
      "metadata": {
        "id": "4336fd1f-c330-4073-9167-12d662e9fb6a"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "num_paraphrases = 25\n",
        "score = 0\n",
        "k = 2\n",
        "euph_corpus['candidates'] = \"\"\n",
        "euph_corpus['top_2'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d27bb917-2350-42f8-8751-af37eaa28263",
      "metadata": {
        "id": "d27bb917-2350-42f8-8751-af37eaa28263",
        "outputId": "e7dc93b4-56ed-4b06-99c3-83cd63f76ac7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 113/1965 [1:14:34<15:49:16, 30.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 279/1965 [2:59:48<12:54:19, 27.56s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 495/1965 [5:05:11<16:02:03, 39.27s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 702/1965 [7:13:27<6:19:51, 18.05s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 827/1965 [8:13:37<6:35:54, 20.87s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 960/1965 [9:13:45<4:19:51, 15.51s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|█████▊    | 1153/1965 [10:57:15<6:27:20, 28.62s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "350\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 1324/1965 [12:20:26<3:28:08, 19.48s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 1382/1965 [12:59:30<5:28:50, 33.84s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Euphemism detected in 420 out of 1965 sentences'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i, row in tqdm(euph_corpus.iterrows(), total=euph_corpus.shape[0]):\n",
        "# uncomment below if resuming from checkpoint\n",
        "#     if (0 < i < 600):\n",
        "#         continue\n",
        "    phrases = euph_corpus.loc[i, 'quality_phrases']\n",
        "    \n",
        "    # Converting string to list IF READING FROM CSV as checkpoint\n",
        "    # phrases = ast.literal_eval(phrases)\n",
        "    \n",
        "    text = euph_corpus.loc[i, 'sentence']\n",
        "    euph = euph_corpus.loc[i, 'keyword']\n",
        "    \n",
        "    top_candidates = get_top_euph_candidates(text, phrases, num_paraphrases, model, \n",
        "                                             sentiment_pack, offensive_pack, show_stats=False)\n",
        "#     print(top_candidates)\n",
        "#     print()\n",
        "    euph_corpus.at[i, 'candidates'] = top_candidates\n",
        "    \n",
        "    # check the top k candidates - this code could use cleaning up\n",
        "    for x in range(0, k):\n",
        "        if (len(top_candidates) == 0):\n",
        "            break\n",
        "        if (len(top_candidates) == 1):\n",
        "            candidate = top_candidates[0][0]\n",
        "            if euph in candidate:\n",
        "                score += 1\n",
        "                if (score % 50 == 0):\n",
        "                    print(score)\n",
        "                euph_corpus.loc[i, 'top_2'] = 1\n",
        "            break\n",
        "        candidate = top_candidates[x][0]\n",
        "        if euph in candidate:\n",
        "            score += 1\n",
        "            if (score % 50 == 0):\n",
        "                print(score)\n",
        "            euph_corpus.loc[i, 'top_2'] = 1\n",
        "            break\n",
        "\n",
        "    if (i == 1382):\n",
        "        break\n",
        "print(\"Euphemism detected in {} out of {} sentences\".format(score, len(euph_corpus)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d32bc341-8a05-4255-8041-fa537db1bf70",
      "metadata": {
        "id": "d32bc341-8a05-4255-8041-fa537db1bf70"
      },
      "source": [
        "# Analytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8482fff-99bb-40ce-ba03-6102aef8ecca",
      "metadata": {
        "id": "f8482fff-99bb-40ce-ba03-6102aef8ecca"
      },
      "outputs": [],
      "source": [
        "euph_corpus.to_csv('results_mar.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "203446a9-3a09-4a9c-992d-e11584cc112f",
      "metadata": {
        "id": "203446a9-3a09-4a9c-992d-e11584cc112f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "euph_corpus = pd.read_csv('results_mar.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ad55f6-b575-4c5a-ad04-b22f989624fc",
      "metadata": {
        "id": "c3ad55f6-b575-4c5a-ad04-b22f989624fc",
        "outputId": "1aa6b68c-d132-446c-bcb0-6493646e7e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "282\n",
            "138\n",
            "91\n"
          ]
        }
      ],
      "source": [
        "import ast\n",
        "# TODO: compute 1st place 2nd place \n",
        "num_first_place = 0\n",
        "num_second_place = 0\n",
        "num_third_place = 0\n",
        "for i, row in euph_corpus.iterrows():\n",
        "    if (i > 1382):\n",
        "        continue\n",
        "    top_2 = euph_corpus.loc[i, 'top_2']\n",
        "    keyword = euph_corpus.loc[i, 'keyword']\n",
        "    candidates = euph_corpus.loc[i, 'candidates']\n",
        "    # Converting string to list\n",
        "    candidates = ast.literal_eval(candidates)\n",
        "    if (top_2 == 1):\n",
        "        if (keyword in candidates[0][0]):\n",
        "            num_first_place += 1\n",
        "        elif (keyword in candidates[1][0]):\n",
        "            num_second_place += 1\n",
        "    elif (len(candidates) > 2):\n",
        "        if (keyword in candidates[2][0]):\n",
        "            num_third_place += 1\n",
        "\n",
        "print(num_first_place)\n",
        "print(num_second_place)\n",
        "print(num_third_place)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f37904c-d414-4ddd-9c6b-ae3055b673af",
      "metadata": {
        "id": "9f37904c-d414-4ddd-9c6b-ae3055b673af",
        "outputId": "21ef2e79-d8eb-479c-e4be-1f275bc5a243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1228\n",
            "57297\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "count = 0\n",
        "tot_p = 0\n",
        "# denote rows where keyword was present in REGULAR phrases\n",
        "for i, row in euph_corpus.iterrows():\n",
        "    if (euph_corpus.loc[i, \"is_euph\"] == 0):\n",
        "        continue\n",
        "    phrases = euph_corpus.loc[i, \"autophrase\"]\n",
        "    # Converting string to list\n",
        "    phrases = ast.literal_eval(phrases)\n",
        "    tot_p += len(phrases)\n",
        "    keyword = euph_corpus.loc[i, 'keyword']\n",
        "    for p in phrases:\n",
        "        p_string = re.sub(r'_', ' ', p)\n",
        "        if keyword in p_string:\n",
        "            #euph_corpus.loc[i, 'keyword_present'] = 1\n",
        "            count += 1\n",
        "            break\n",
        "            \n",
        "print(count)\n",
        "print(tot_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b407547-1567-4d49-b49d-3836628bd80c",
      "metadata": {
        "id": "5b407547-1567-4d49-b49d-3836628bd80c",
        "outputId": "f51e672d-ba4d-4c9b-e0a4-02060409cbc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1228\n",
            "32359\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "count = 0\n",
        "tot_q = 0\n",
        "# denote rows where keyword was present in quality phrases\n",
        "euph_corpus['keyword_present'] = 0\n",
        "for i, row in euph_corpus.iterrows():\n",
        "    if (euph_corpus.loc[i, \"is_euph\"] == 0):\n",
        "        continue\n",
        "    quality_phrases = euph_corpus.loc[i, \"quality_phrases\"]\n",
        "    # Converting string to list\n",
        "    quality_phrases = ast.literal_eval(quality_phrases)\n",
        "    tot_q += len(quality_phrases)\n",
        "    keyword = euph_corpus.loc[i, 'keyword']\n",
        "    for q in quality_phrases:\n",
        "        q_string = re.sub(r'_', ' ', q)\n",
        "        if keyword in q_string:\n",
        "            euph_corpus.loc[i, 'keyword_present'] = 1\n",
        "            count += 1\n",
        "            break\n",
        "\n",
        "print(count)\n",
        "print(tot_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9866f40c-d2cf-4309-ab0f-1770c171aec4",
      "metadata": {
        "id": "9866f40c-d2cf-4309-ab0f-1770c171aec4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "MGEuphs(5)(1).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}